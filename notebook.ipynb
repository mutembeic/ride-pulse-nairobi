{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4458e6",
   "metadata": {},
   "source": [
    "# RidePulse Nairobi\n",
    "## Predictive Demand Hotspots for Boda-Boda Riders\n",
    "\n",
    "RidePulse Nairobi is a data science project aimed at solving a core economic challenge of inefficient positioning and excessive idle time for thousands of independent boda-boda riders in Nairobi. By analyzing historical ride data, we will build a machine learning model to predict high-demand \"hotspots\" across the city in real-time. The final output will be a simple, interactive heat map prototype that guides riders to areas with the highest probability of securing a customer, directly translating into reduced fuel costs, less idle time, and increased daily income.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Nairobi's boda-boda riders operate in a hyper-competitive market. Their income is directly proportional to the number of trips they complete. However, they lack predictive tools, forcing them to rely on gut instinct and experience to find customers. This leads to critical inefficiencies such as:\n",
    "\n",
    "1. Wasted Fuel and Time: Riders spend significant portions of their day \"cruising empty\" in search of passengers.\n",
    "\n",
    "2. Oversupply and Competition: Riders often congregate in traditionally \"busy\" areas (e.g., CBD, Westlands), only to find intense competition and long waits.\n",
    "\n",
    "3. Missed Opportunities: A lucrative ride request might be available just a few blocks away in a non-obvious location, but the rider has no way of knowing.\n",
    "\n",
    "This information gap puts a hard ceiling on a rider's potential earnings and operational efficiency.\n",
    "\n",
    "\n",
    "\n",
    "### Proposed Solution: Data-Driven Positioning\n",
    "\n",
    "We propose building a predictive system that transforms raw data into actionable intelligence. The system will:\n",
    "\n",
    "1. Forecast Demand: Use a machine learning model to predict the number of ride requests for specific zones across Nairobi for any given hour and day.\n",
    "\n",
    "2. Visualize Insights: Translate these predictions into a simple, color-coded heat map overlaid on a map of Nairobi.\n",
    "\n",
    "        - Red/Orange: \"Hot Zone\" - Go here for a high chance of a ride.\n",
    "\n",
    "        - Yellow: \"Warm Zone\" - Moderate demand.\n",
    "\n",
    "        - Blue/Clear: \"Cold Zone\" - Avoid waiting here.\n",
    "\n",
    "3. Empower Riders: Provide a simple, visual tool (simulated via a web app) that answers the rider's most important question: \"Where should I be right now to find my next customer?\"\n",
    "\n",
    "\n",
    "### Key Objectives\n",
    "\n",
    "1. Process and transform raw ride data into a structured feature set by engineering time-based features and implementing Uber's H3 spatial indexing to grid the city into hexagonal zones.\n",
    "\n",
    "2. Develop a regression model (e.g., LightGBM) to accurately forecast ride demand per zone per hour, aiming for a predictive accuracy (R-squared) of over 75%.\n",
    "\n",
    "3. Build an interactive prototype using Streamlit and Folium that displays the demand forecast as an intuitive heat map, proving the project's real-world applicability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf4d3b",
   "metadata": {},
   "source": [
    "## Data Understanding \n",
    "\n",
    "We will use the Sendy Logistics Challenge dataset available on Zindi. It contains over 20,000 real-world boda-boda delivery records from Nairobi, including precise pickup timestamps and latitude/longitude coordinates. Direct Link: https://zindi.africa/competitions/sendy-logistics-challenge/data\n",
    "\n",
    "Tech Stack:\n",
    "\n",
    "        - Language: Python\n",
    "        \n",
    "        - Core Libraries: Pandas, NumPy, Scikit-learn, LightGBM\n",
    "        \n",
    "        - Geospatial: Geopandas, H3-py, Folium\n",
    "        \n",
    "        - Prototyping: Streamlit\n",
    "\n",
    "#### Success Metrics\n",
    "\n",
    "We will measure success both technically and practically:\n",
    "\n",
    "1. Technical Metric (MAE): The model's Mean Absolute Error should be less than 3 rides, meaning our predictions are, on average, very close to the actual demand.\n",
    "\n",
    "2. Business Metric (Hotspot Precision): The model must correctly identify at least 8 out of the 10 actual busiest zones during peak hours, proving its effectiveness in finding profitable locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcc3d3",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e90828f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5652346",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5dcce",
   "metadata": {},
   "source": [
    "Looking through the dataset We begin by loading the dataset and examining its structure, columns, and a few sample rows to get a feel for the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b46d3",
   "metadata": {},
   "source": [
    "From the cell above, we see see that two columns (Temperature and Precipitation in millimeters) have null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcd8a0",
   "metadata": {},
   "source": [
    "**Converting the 'Placement_Datetime' column to date time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert the columns to be date time assuming the month of jan\n",
    "placeholder_month = 1\n",
    "placeholder_year = 2019\n",
    "\n",
    "df['Placement_Datetime'] = pd.to_datetime(\n",
    "    df['Placement - Day of Month'].astype(str) + '-' +\n",
    "    str(placeholder_month) + '-' +\n",
    "    str(placeholder_year) + ' ' +\n",
    "    df['Placement - Time'],\n",
    "    format='%d-%m-%Y %I:%M:%S %p'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75794cdd",
   "metadata": {},
   "source": [
    "The columns we are going to use in this project are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "essential_cols = [\n",
    "    'Order No',            \n",
    "    'Placement_Datetime',  \n",
    "    'Personal or Business',\n",
    "    'Platform Type',\n",
    "    'Pickup Lat', \n",
    "    'Pickup Long'          \n",
    "]\n",
    "\n",
    "\n",
    "df_focused = df[essential_cols].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_focused.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a07f3d",
   "metadata": {},
   "source": [
    "### Dispalying a Map of Pickup Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477db213",
   "metadata": {},
   "source": [
    "Here, we install and import **folium**, a Python library used to create interactive maps directly in Jupyter Notebooks. We create an interactive heat map showing the density of pickup locations in Nairobi, based on latitude and longitude coordinates in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed06192",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fe806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "#Visualize the pick up location using latitude and longitude\n",
    "heat_data = df_focused[['Pickup Lat', 'Pickup Long']].values.tolist()\n",
    "nairobi_map = folium.Map(location=[-1.286389, 36.817223], zoom_start=12)\n",
    "HeatMap(heat_data).add_to(nairobi_map)\n",
    "print(\"Displaying Heat Map of Pickup Locations...\")\n",
    "nairobi_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the  numerical features from the date\n",
    "df_focused['hour_of_day'] = df_focused['Placement_Datetime'].dt.hour\n",
    "df_focused['day_of_week'] = df_focused['Placement_Datetime'].dt.dayofweek\n",
    "df_focused.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342fc82",
   "metadata": {},
   "source": [
    "### H3 Geospatial\n",
    "\n",
    "H3 is an open-source geospatial indexing system developed by Uber. It divides the surface of the Earth into hexagonal cells, which makes it easier to analyze and visualize geographic data efficiently and accurately. In the cell below, we use H3 to convert each pickup location's latitude and longitude into a unique H3 hexagonal cell ID, and store the result in a new column called **h3_cell**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the longitude and latitude into h3 cells\n",
    "!pip install h3\n",
    "\n",
    "import h3\n",
    "\n",
    "H3_RESOLUTION = 12\n",
    "\n",
    "def latlon_to_h3(row):\n",
    "    return h3.latlng_to_cell(row['Pickup Lat'], row['Pickup Long'], H3_RESOLUTION)\n",
    "\n",
    "df_focused['h3_cell'] = df_focused.apply(latlon_to_h3, axis=1)\n",
    "df_focused.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a564d",
   "metadata": {},
   "source": [
    "Label Encoding for binary categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We binary encode the personal/business column\n",
    "df_focused['is_business'] = df_focused['Personal or Business'].apply(lambda x: 1 if x == 'Business' else 0)\n",
    "#preview\n",
    "df_focused[['Personal or Business', 'is_business']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fe8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_focused.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94596f2",
   "metadata": {},
   "source": [
    "Below, we group ride request data by location (H3 cell), day of the week, and hour of the day to prepare our dataset for analysis. We calculate how many ride requests occurred (demand_count), what proportion were business rides (business_ratio) and then merge both metrics into one DataFrame (df_model_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try to group by location cell and the date- day and hour of the week\n",
    "demand_counts = df_focused.groupby(['h3_cell', 'day_of_week', 'hour_of_day']).size().reset_index(name='demand_count')\n",
    "business_proportion = df_focused.groupby(['h3_cell', 'day_of_week', 'hour_of_day'])['is_business'].mean().reset_index(name='business_ratio')\n",
    "\n",
    "#we merge both dataframes\n",
    "df_model_ready = pd.merge(demand_counts, business_proportion, on=['h3_cell', 'day_of_week', 'hour_of_day'])\n",
    "\n",
    "print(df_model_ready.shape)\n",
    "df_model_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002f27c",
   "metadata": {},
   "source": [
    "Below, we convert the h3_cell column into a categorical data type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf100781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model_ready['h3_cell'] = df_model_ready['h3_cell'].astype('category')\n",
    "df_model_ready.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed978b",
   "metadata": {},
   "source": [
    "**Visualizing patterns in ride demand across hours of the day and days of the week using bar plots**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ea058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by hour and day to see the patterns\n",
    "hourly_demand = df_model_ready.groupby('hour_of_day')['demand_count'].mean()\n",
    "daily_demand = df_model_ready.groupby('day_of_week')['demand_count'].mean()\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "#creating the plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "#hourly Demand Plot\n",
    "sns.barplot(x=hourly_demand.index, y=hourly_demand.values, ax=ax1, palette='viridis')\n",
    "ax1.set_title('Average Demand by Hour of Day')\n",
    "ax1.set_xlabel('Hour')\n",
    "ax1.set_ylabel('Average Demand')\n",
    "\n",
    "#Daily Demand Plot\n",
    "sns.barplot(x=day_names, y=daily_demand.values, ax=ax2, palette='plasma')\n",
    "ax2.set_title('Average Demand by Day of Week')\n",
    "ax2.set_xlabel('Day')\n",
    "ax2.set_ylabel('Average Demand')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3988a4",
   "metadata": {},
   "source": [
    "**Selecting Features and Target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['h3_cell', 'day_of_week', 'hour_of_day', 'business_ratio']\n",
    "target = 'demand_count'\n",
    "\n",
    "X = df_model_ready[features]\n",
    "y = df_model_ready[target]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_val.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c68bf0",
   "metadata": {},
   "source": [
    "## Modelling: Ensemble Methods\n",
    "### LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "#instaniate the model\n",
    "lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "#fitting the model\n",
    "lgbm.fit(X_train, y_train, categorical_feature=['h3_cell'])\n",
    "\n",
    "print(\"Model Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"A visualization of the feature importance\")\n",
    "\n",
    "lgb.plot_importance(lgbm, height=0.9, figsize=(10, 6))\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = lgbm.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Results\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# comparing few predictions vs the actual values\n",
    "comparison_df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred})\n",
    "print(\"\\nSample of Actual vs. Predicted demand:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258dce6",
   "metadata": {},
   "source": [
    "### Observations from Sample Predictions\n",
    "* **MAE of 0.41:** The model's predictions are, on average, within 0.41 units of the actual `demand_count`.\n",
    "* **R-squared of 0.86:** The model effectively explains 86% of the variability observed in the `demand_count`.\n",
    "* **Accuracy for Low Demand:** For `Actual` values of 1, the model often predicts values very close to 1 (e.g., 1.017240, 0.993542, 1.050034). This suggests good accuracy for low demand scenarios.\n",
    "* **Variability for Moderate Demand:** For `Actual` values of 2, the predictions are a bit more varied (e.g., 0.722886, 1.018311). The first one (0.72) shows a larger error compared to others.\n",
    "* **Challenges with Higher Demand:** For `Actual` values like 3 and 4, the model seems to struggle more. For `Actual` 4, it predicted 1.517652, which is quite far off. However, for `Actual` 3, it predicted 3.398914, which is very close. This indicates that while the model performs well on average, its accuracy might vary for different demand levels, particularly for higher ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b5eb2",
   "metadata": {},
   "source": [
    "### Xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61791c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install XGBoost\n",
    "#!pip install --upgrade xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train_xgb = X_train.copy()\n",
    "X_val_xgb = X_val.copy()\n",
    "\n",
    "#converting the categorical h3 column into integer for the model\n",
    "X_train_xgb['h3_cell'] = X_train_xgb['h3_cell'].cat.codes\n",
    "X_val_xgb['h3_cell'] = X_val_xgb['h3_cell'].cat.codes\n",
    "\n",
    "#initialize and train model\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True,  \n",
    "    random_state=42\n",
    ")\n",
    "xgb_reg.fit(np.array(X_train_xgb), y_train)\n",
    "print ('finished fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82979f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the XGBoost Model ---\n",
    "y_pred_xgb = xgb_reg.predict(np.array(X_val_xgb))\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "\n",
    "print(\"\\n--- XGBoost Evaluation Results ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_xgb:.2f}\")\n",
    "\n",
    "# comparing few predictions vs the actual values\n",
    "comparison_df_xgb = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred_xgb})\n",
    "print(\"\\nSample of Actual vs. Predicted demand:\")\n",
    "print(comparison_df_xgb.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea7020",
   "metadata": {},
   "source": [
    "## Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139989f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['h3_cell']\n",
    "\n",
    "cat_reg = CatBoostRegressor(\n",
    "    iterations=500,  \n",
    "    verbose=0,      \n",
    "    cat_features=cat_features,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training CatBoost Model\")\n",
    "cat_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the CatBoost Model\n",
    "y_pred_cat = cat_reg.predict(X_val)\n",
    "mae_cat = mean_absolute_error(y_val, y_pred_cat)\n",
    "r2_cat = r2_score(y_val, y_pred_cat)\n",
    "\n",
    "print(\"CatBoost Evaluation Results\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_cat:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_cat:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cac2fd",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE) 0.40: On average, our model's predictions are only 0.40 off from the actual demand values.\n",
    "\n",
    "R-squared (R²) 0.87: The model explains 87% of the variability in demand count, which is high and suggests the model captures the demand patterns really well.\n",
    "\n",
    "CatBoost did better since it automatically handles categorical features like h3_cell efficiently without one-hot encoding and works great on tabular data with mixed types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c9cd5",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The best type of neural network to start with, given this structure, is a Multi-Layer Perceptron (MLP), also known as a Feed-Forward Neural Network, enhanced with Embedding Layers for your categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb67e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Copy datasets\n",
    "X_train_nn = X_train.copy()\n",
    "X_val_nn = X_val.copy()\n",
    "\n",
    "# category mapping\n",
    "combined = pd.concat([X_train['h3_cell'], X_val['h3_cell']], axis=0).astype('category')\n",
    "combined = combined.cat.set_categories(combined.unique())  # optional but safe\n",
    "\n",
    "# Assign consistent codes\n",
    "X_train_nn['h3_cell'] = combined[:len(X_train)].cat.codes\n",
    "X_val_nn['h3_cell'] = combined[len(X_train):].cat.codes\n",
    "\n",
    "# unique counts for embeddings\n",
    "num_h3_cells = combined.nunique()+1\n",
    "num_days = 7\n",
    "num_hours = 24\n",
    "\n",
    "# Embedding dimensions\n",
    "h3_embedding_dim = min(50, num_h3_cells // 2)\n",
    "day_embedding_dim = min(50, num_days // 2)\n",
    "hour_embedding_dim = min(50, num_hours // 2)\n",
    "\n",
    "# Scaling the numerical column\n",
    "scaler = MinMaxScaler()\n",
    "X_train_nn['business_ratio'] = scaler.fit_transform(X_train_nn[['business_ratio']])\n",
    "X_val_nn['business_ratio'] = scaler.transform(X_val_nn[['business_ratio']])\n",
    "\n",
    "print(X_val_nn.info())\n",
    "X_train_nn.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cdb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input layers\n",
    "input_h3 = layers.Input(shape=(1,), name='h3_cell')\n",
    "input_day = layers.Input(shape=(1,), name='day_of_week')\n",
    "input_hour = layers.Input(shape=(1,), name='hour_of_day')\n",
    "input_num = layers.Input(shape=(1,), name='business_ratio')\n",
    "\n",
    "# Embedding layers\n",
    "embed_h3 = layers.Embedding(input_dim=num_h3_cells, output_dim=h3_embedding_dim)(input_h3)\n",
    "embed_day = layers.Embedding(input_dim=num_days, output_dim=day_embedding_dim)(input_day)\n",
    "embed_hour = layers.Embedding(input_dim=num_hours, output_dim=hour_embedding_dim)(input_hour)\n",
    "\n",
    "# Flatten embeddings\n",
    "flat_h3 = layers.Flatten()(embed_h3)\n",
    "flat_day = layers.Flatten()(embed_day)\n",
    "flat_hour = layers.Flatten()(embed_hour)\n",
    "\n",
    "# Concatenate all features\n",
    "x = layers.Concatenate()([flat_h3, flat_day, flat_hour, input_num])\n",
    "\n",
    "# Dense layers\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(1)(x) \n",
    "\n",
    "# Compile model\n",
    "model = keras.Model(inputs=[input_h3, input_day, input_hour, input_num], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val_nn['h3_cell'].tail(10))\n",
    "X_train_nn['h3_cell'].tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        'h3_cell': X_train_nn['h3_cell'],\n",
    "        'day_of_week': X_train_nn['day_of_week'],\n",
    "        'hour_of_day': X_train_nn['hour_of_day'],\n",
    "        'business_ratio': X_train_nn['business_ratio']\n",
    "    },\n",
    "    y=y_train,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'h3_cell': X_val_nn['h3_cell'],\n",
    "            'day_of_week': X_val_nn['day_of_week'],\n",
    "            'hour_of_day': X_val_nn['hour_of_day'],\n",
    "            'business_ratio': X_val_nn['business_ratio']\n",
    "        },\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37434786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on validation set\n",
    "y_pred = model.predict({\n",
    "    'h3_cell': X_val_nn['h3_cell'],\n",
    "    'day_of_week': X_val_nn['day_of_week'],\n",
    "    'hour_of_day': X_val_nn['hour_of_day'],\n",
    "    'business_ratio': X_val_nn['business_ratio']\n",
    "})\n",
    "\n",
    "# Evaluate\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"R²:\", r2_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd332955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ridepulse-venv",
   "language": "python",
   "name": "ridepulse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
